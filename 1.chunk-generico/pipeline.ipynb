{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f63e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load .env from parent directory if needed\n",
    "env_path = Path(__file__).resolve().parent.parent / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Access variables\n",
    "OPENAI_API_KEY        = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_INDEX_NAME   = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "PINECONE_HOST         = os.getenv(\"PINECONE_HOST\")\n",
    "PINECONE_API_KEY      = os.getenv(\"PINECONE_API_KEY\")\n",
    "GEMINI_API_KEY        = os.getenv(\"GEMINI_API_KEY\")\n",
    "K_RETRIEVE            = int(os.getenv(\"K_RETRIEVE\", 5))  # default to 5\n",
    "OPENAI_API_KEY        = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import pinecone\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• INIT MODELS â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")  # 1024-D\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• LOAD CORPUS â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def split_text_into_chunks(text, chunk_size=900, overlap=100):\n",
    "    chunks: List = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk.strip())\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Read the full text\n",
    "with open(\"constitucion_nacional.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Split into chunks\n",
    "chunks = split_text_into_chunks(text)\n",
    "\n",
    "# Write the chunks to a new file\n",
    "with open(\"constitucion_static_chunks.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    out_file.write(\"\\n\\n\".join(chunks))\n",
    "\n",
    "print(f\"Generated {len(chunks)} chunks.\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• EMBEDDING FUNCTION (E5) â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    formatted = [f\"passage: {text}\" for text in texts]\n",
    "    return model.encode(formatted, show_progress_bar=True)\n",
    "\n",
    "print(\"ğŸ”§  Generating embeddings â€¦\")\n",
    "EMBEDS = embed_texts(chunks)\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• PINECONE SETUP â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(name=PINECONE_INDEX_NAME, host=PINECONE_HOST)\n",
    "\n",
    "def upsert_vectors(texts: List[str],\n",
    "                   vecs: List[List[float]],\n",
    "                   batch: int = 100):\n",
    "    for i in tqdm(range(0, len(texts), batch), desc=\"â¬†ï¸  Upserting\"):\n",
    "        batch_vecs = [\n",
    "            {\n",
    "                \"id\": f\"id-{j}\",\n",
    "                \"values\": vecs[j],\n",
    "                \"metadata\": {\"text\": texts[j]}\n",
    "            }\n",
    "            for j in range(i, min(i + batch, len(texts)))\n",
    "        ]\n",
    "        index.upsert(vectors=batch_vecs)\n",
    "\n",
    "upsert_vectors(chunks, EMBEDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• RETRIEVE FUNCTION  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def retrieve(query: str, k: int = K_RETRIEVE) -> List[str]:\n",
    "    query_vec = model.encode(f\"query: {query}\")\n",
    "    res = index.query(vector=query_vec.tolist(), top_k=k, include_metadata=True)\n",
    "    return [m.metadata[\"text\"] for m in res.matches]\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• GEMINI PRO RAG â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "gemini = genai.GenerativeModel(model_name=\"gemini-2.0-flash\") \n",
    "\n",
    "def rag_answer(question: str) -> str:\n",
    "    context = \"\\n\\n\".join(retrieve(question))\n",
    "    prompt  = f\"Contexto:\\n{context}\\n\\nPregunta: {question}\\nRespuesta:\"\n",
    "    return gemini.generate_content(prompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• TEST IT â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "q = \"Â¿CuÃ¡les son las atribuciones del presidente de la Argentina?\"\n",
    "print(\"\\nğŸ” Pregunta:\", q)\n",
    "print(\"\\nğŸ§  Respuesta (Gemini):\\n\", rag_answer(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from ragas.evaluation import evaluate\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def load_reference_answers(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return {item[\"question\"]: item[\"answer\"] for item in data}\n",
    "\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ConstrucciÃ³n del dataset para RAGAS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def prepare_ragas_dataset(reference_answers: dict, tag: str = \"\") -> Dataset:\n",
    "    records = []\n",
    "    for question, reference in tqdm(reference_answers.items(), desc=f\"ğŸ§ª Procesando {tag}\"):\n",
    "        retrieved = retrieve(question)\n",
    "        answer = rag_answer(question)\n",
    "        records.append({\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": retrieved,\n",
    "            \"response\": answer,\n",
    "            \"reference\": reference\n",
    "        })\n",
    "        time.sleep(4)\n",
    "\n",
    "\n",
    "reference_easy = load_reference_answers(\"../easy_questions.json\")\n",
    "reference_hard = load_reference_answers(\"../hard_questions.json\")\n",
    "\n",
    "# Prepare datasets\n",
    "dataset_easy = prepare_ragas_dataset(reference_easy, tag=\"fÃ¡ciles\")\n",
    "dataset_hard = prepare_ragas_dataset(reference_hard, tag=\"difÃ­ciles\")\n",
    "\n",
    "# Save to JSON\n",
    "dataset_easy.to_json(\"ragas_easy_2.json\", orient=\"records\", lines=False)\n",
    "dataset_hard.to_json(\"ragas_hard_2.json\", orient=\"records\", lines=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.embeddings.base import LangchainEmbeddingsWrapper\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class CustomE5Embedding(LangchainEmbeddingsWrapper):\n",
    "    def __init__(self, model_name=\"intfloat/multilingual-e5-large\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_query(self, texts):\n",
    "        texts = [f\"query: {text}\" for text in texts]\n",
    "        return self.model.encode(texts, convert_to_tensor=False)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        return self.model.encode(texts, convert_to_tensor=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    LLMContextPrecisionWithoutReference,\n",
    "    LLMContextPrecisionWithReference,\n",
    "    NonLLMContextPrecisionWithReference,\n",
    "\n",
    "\n",
    "    ResponseRelevancy,\n",
    "    LLMContextRecall,\n",
    "    Faithfulness\n",
    ")\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.embeddings.base import embedding_factory\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "ragas_easy:Dataset = load_dataset(\"json\", data_files=\"ragas_easy.json\", split=\"train\")\n",
    "ragas_hard:Dataset = load_dataset(\"json\", data_files=\"ragas_hard.json\", split=\"train\")\n",
    "\n",
    "# Run config\n",
    "run_config = RunConfig()\n",
    "custom_embeddings = CustomE5Embedding() \n",
    "\n",
    "metrics = [\n",
    "    LLMContextPrecisionWithReference(),\n",
    "    LLMContextRecall(),\n",
    "    Faithfulness()\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“Š Evaluando preguntas FÃCILES:\")\n",
    "result_easy = evaluate(\n",
    "    ragas_easy,\n",
    "    metrics=metrics,\n",
    "    run_config=run_config,\n",
    "    batch_size=1\n",
    ")\n",
    "print(\"âœ… Resultados EASY:\", result_easy)\n",
    "\n",
    "print(\"\\nğŸ“Š Evaluando preguntas DIFÃCILES:\")\n",
    "result_hard = evaluate(\n",
    "    ragas_hard,\n",
    "    metrics=metrics,\n",
    "    run_config=run_config,\n",
    "    batch_size=1\n",
    ")\n",
    "print(\"âœ… Resultados HARD:\", result_hard)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cec2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.mean(result_easy['llm_context_precision_with_reference']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85297e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "csv_path = Path(\"../results.csv\")\n",
    "write_header = not csv_path.exists()\n",
    "\n",
    "metric_names = [\n",
    "    \"llm_context_precision_with_reference\",\n",
    "    \"context_recall\",\n",
    "    \"faithfulness\"\n",
    "]\n",
    "\n",
    "easy_scores = [np.mean(result_easy[m]) for m in metric_names]\n",
    "hard_scores = [np.mean(result_hard[m]) for m in metric_names]\n",
    "\n",
    "with open(csv_path, mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    if write_header:\n",
    "        writer.writerow([\"experiment\", \"dataset\"] + metric_names)\n",
    "\n",
    "    writer.writerow([\"experiment_1\", \"easy\"] + easy_scores)\n",
    "    writer.writerow([\"experiment_1\", \"hard\"] + hard_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "juegos-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
