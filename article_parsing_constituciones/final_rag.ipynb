{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4035c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load .env from current working directory (Jupyter notebooks do not define __file__)\n",
    "env_path = Path().resolve() / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Access variables\n",
    "OPENAI_API_KEY        = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_INDEX_NAME   = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "PINECONE_HOST         = os.getenv(\"PINECONE_HOST\")\n",
    "PINECONE_API_KEY      = os.getenv(\"PINECONE_API_KEY\")\n",
    "GEMINI_API_KEY        = os.getenv(\"GEMINI_API_KEY\")\n",
    "K_RETRIEVE            = int(os.getenv(\"K_RETRIEVE\", 5))  # default to 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fec3bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š  Loaded 5,689 artÃ­culos de 24 provincias\n",
      "ğŸ”§  Generating embeddings â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ§  Embedding batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 178/178 [06:52<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¤  Uploading to Pinecone â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â¬†ï¸  Upserting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:51<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pinecone\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• CONFIGS â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "JSON_DIR = \".\"\n",
    "EMBEDDING_DIM = 1024\n",
    "MAX_WORDS = 1000\n",
    "BATCH_SIZE_EMBED = 32\n",
    "BATCH_SIZE_UPSERT = 100\n",
    "\n",
    "# Configura tus claves de API\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• INIT MODELS â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "model = SentenceTransformer(\"dariolopez/bge-m3-es-legal-tmp-6\")  # 1024-D\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• LOAD CORPUS WITH METADATA â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def load_texts_with_province(json_folder: str) -> Tuple[List[str], List[str]]:\n",
    "    textos, provincias = [], []\n",
    "    for filename in sorted(os.listdir(json_folder)):\n",
    "        if filename.endswith(\".json\") and filename[:2].isdigit() and 2 <= int(filename[:2]) <= 26:\n",
    "            filepath = os.path.join(json_folder, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                for entry in data:\n",
    "                    if \"text\" in entry and \"province\" in entry:\n",
    "                        textos.append(entry[\"text\"].strip())\n",
    "                        provincias.append(entry[\"province\"].strip())\n",
    "    return textos, provincias\n",
    "\n",
    "ARTICULOS, PROVINCIAS = load_texts_with_province(JSON_DIR)\n",
    "print(f\"ğŸ“š  Loaded {len(ARTICULOS):,} artÃ­culos de {len(set(PROVINCIAS)):,} provincias\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• TRUNCATE LONG TEXTS â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def truncate_text(text: str, max_words: int = MAX_WORDS) -> str:\n",
    "    words = text.split()\n",
    "    return \" \".join(words[:max_words])\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• EMBEDDING FUNCTION (robusta) â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def embed_texts(texts: List[str], batch_size: int = BATCH_SIZE_EMBED) -> List[List[float]]:\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"ğŸ§  Embedding batches\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        formatted = [f\"passage: {truncate_text(text)}\" for text in batch_texts]\n",
    "        try:\n",
    "            embeddings = model.encode(formatted, show_progress_bar=False)\n",
    "            all_embeddings.extend(embeddings)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"âŒ Error en batch {i}-{i+batch_size}: {e}\")\n",
    "            continue\n",
    "    return all_embeddings\n",
    "\n",
    "print(\"ğŸ”§  Generating embeddings â€¦\")\n",
    "EMBEDS = embed_texts(ARTICULOS)\n",
    "assert len(EMBEDS[0]) == EMBEDDING_DIM, \"âŒ Embedding dim mismatch!\"\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• PINECONE SETUP â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(name=PINECONE_INDEX_NAME, host=PINECONE_HOST)\n",
    "\n",
    "def upsert_vectors(texts: List[str],\n",
    "                   provinces: List[str],\n",
    "                   vecs: List[List[float]],\n",
    "                   batch: int = BATCH_SIZE_UPSERT):\n",
    "    for i in tqdm(range(0, len(texts), batch), desc=\"â¬†ï¸  Upserting\"):\n",
    "        batch_vecs = [\n",
    "            {\n",
    "                \"id\": f\"id-{j}\",\n",
    "                \"values\": vecs[j],\n",
    "                \"metadata\": {\n",
    "                    \"text\": texts[j],\n",
    "                    \"province\": provinces[j]\n",
    "                }\n",
    "            }\n",
    "            for j in range(i, min(i + batch, len(texts)))\n",
    "        ]\n",
    "        index.upsert(vectors=batch_vecs)\n",
    "\n",
    "print(\"ğŸ“¤  Uploading to Pinecone â€¦\")\n",
    "upsert_vectors(ARTICULOS, PROVINCIAS, EMBEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e153350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• RETRIEVE FUNCTION (mÃºltiples provincias) â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def retrieve(query: str, provinces: List[str], k: int = K_RETRIEVE) -> List[str]:\n",
    "    query_vec = model.encode(f\"query: {query}\")\n",
    "    province_filter = {\"province\": {\"$in\": provinces}} if provinces else {}\n",
    "\n",
    "    res = index.query(\n",
    "        vector=query_vec.tolist(),\n",
    "        top_k=k,\n",
    "        include_metadata=True,\n",
    "        filter=province_filter\n",
    "    )\n",
    "    return [m.metadata[\"text\"] for m in res.matches]\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• GEMINI PRO RAG â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "gemini = genai.GenerativeModel(model_name=\"gemini-2.0-flash\")\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Eres un/a **abogado/a constitucionalista argentino/a**.\n",
    "Tu tarea es **contestar en UNA sola frase** y **exclusivamente** con la\n",
    "informaciÃ³n que aparece dentro de las etiquetas <context></context>.\n",
    "\n",
    "Reglas de oro (cÃºmplelas al pie de la letra):\n",
    "\n",
    "1. Si la respuesta estÃ¡ en el contexto, da la soluciÃ³n **exactamente** como\n",
    "   figura allÃ­, sin agregar ni quitar nada relevante.\n",
    "2. Al final de la frase, escribe entre parÃ©ntesis el/los nÃºmero(s) de\n",
    "   artÃ­culo(s) que sustenten la respuesta â€“por ejemplo: **(art.â€¯14)**.\n",
    "   - Si el fragmento de contexto trae algo como â€œArtÃ­culo 14â€¯bisâ€, ponlo igual: **(art.â€¯14â€¯bis)**.\n",
    "3. Si la informaciÃ³n **no** aparece en el contexto, contesta **exactamente**:\n",
    "   > No tengo informaciÃ³n sobre esto.\n",
    "4. No inventes datos, no cites fuentes externas, no expliques tu razonamiento.\n",
    "5. Responde en espaÃ±ol neutro y evita tecnicismos innecesarios.\n",
    "6. Si no sabes la respuesta, responde 'no tengo informaciÃ³n sobre esto'.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Pregunta: {question}\n",
    "Respuesta:\n",
    "\"\"\".strip()\n",
    "\n",
    "def rag_answer(question: str, provinces: List[str]) -> str:\n",
    "    context = \"\\n\\n\".join(retrieve(question, provinces=provinces))\n",
    "    prompt  = PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "    return gemini.generate_content(prompt).text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7ae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Pregunta: Que informacion hay sobre el sistema de salud?\n",
      "\n",
      "ğŸ§  Respuesta (Gemini):\n",
      " El Sistema de Salud Provincial se integrarÃ¡ con los servicios pÃºblicos de gestiÃ³n estatal y de gestiÃ³n privada (no se explicita el nÃºmero de artÃ­culo).\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• TEST IT â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "q = \"Que informacion hay sobre el sistema de salud?\"\n",
    "print(\"\\nğŸ” Pregunta:\", q)\n",
    "print(\"\\nğŸ§  Respuesta (Gemini):\\n\", rag_answer(q, [\"La Rioja\", \"La NaciÃ³n Argentina\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ff47a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
