{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚  Loaded 199 artículos\n",
      "🔧  Generating embeddings …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:08<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤  Uploading to Pinecone …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⬆️  Upserting: 100%|██████████| 2/2 [00:02<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "PINECONE_INDEX_NAME   = \"6-prompt\"\n",
    "PINECONE_HOST         = \"https://6-prompt-1ov95ew.svc.aped-4627-b74a.pinecone.io\"\n",
    "\n",
    "JSON_PATH = \"incisos-chunks.json\"\n",
    "\n",
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║                 CONFIG SECTION                 ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "PINECONE_API_KEY      = \"pcsk_5ux2kL_6W6FVpmRbdLXxbRS4zZkTxZwR2JWy9MZJ3aGZGfPWko1JPfjDFqGpsMUXDWZMfd\"\n",
    "GEMINI_API_KEY        = \"AIzaSyCAXNRkdO0REJJ66-8u2ubQKMgco9NmaXc\"\n",
    "K_RETRIEVE            = 10                                     \n",
    "# ╔════════════════════════════════════════════════╗\n",
    "# ║                LIBRARIES                       ║\n",
    "# ╚════════════════════════════════════════════════╝\n",
    "# !pip install sentence-transformers pinecone-client google-generativeai tqdm\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import pinecone\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ════════════════ INIT MODELS ════════════════\n",
    "model = SentenceTransformer(\"dariolopez/bge-m3-es-legal-tmp-6\")  # 1024-D\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# ════════════════ LOAD CORPUS ════════════════\n",
    "def load_texts_from_json(json_path: str) -> List[str]:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return [entry[\"text\"].strip() for entry in data if \"text\" in entry]\n",
    "\n",
    "ARTICULOS = load_texts_from_json(JSON_PATH)\n",
    "print(f\"📚  Loaded {len(ARTICULOS):,} artículos\")\n",
    "\n",
    "\n",
    "# ════════════════ EMBEDDING FUNCTION (E5) ════════════════\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    formatted = [f\"passage: {text}\" for text in texts]\n",
    "    return model.encode(formatted, show_progress_bar=True)\n",
    "\n",
    "print(\"🔧  Generating embeddings …\")\n",
    "EMBEDS = embed_texts(ARTICULOS)\n",
    "assert len(EMBEDS[0]) == 1024, \"❌ Embedding dim mismatch!\"\n",
    "\n",
    "\n",
    "# ════════════════ PINECONE SETUP ════════════════\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(name=PINECONE_INDEX_NAME, host=PINECONE_HOST)\n",
    "\n",
    "def upsert_vectors(texts: List[str],\n",
    "                   vecs: List[List[float]],\n",
    "                   batch: int = 100):\n",
    "    for i in tqdm(range(0, len(texts), batch), desc=\"⬆️  Upserting\"):\n",
    "        batch_vecs = [\n",
    "            {\n",
    "                \"id\": f\"id-{j}\",\n",
    "                \"values\": vecs[j],\n",
    "                \"metadata\": {\"text\": texts[j]}\n",
    "            }\n",
    "            for j in range(i, min(i + batch, len(texts)))\n",
    "        ]\n",
    "        index.upsert(vectors=batch_vecs)\n",
    "\n",
    "print(\"📤  Uploading to Pinecone …\")\n",
    "upsert_vectors(ARTICULOS, EMBEDS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37bafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ════════════════ RETRIEVE FUNCTION  ════════════════\n",
    "def retrieve(query: str, k: int = K_RETRIEVE) -> List[str]:\n",
    "    query_vec = model.encode(f\"query: {query}\")\n",
    "    res = index.query(vector=query_vec.tolist(), top_k=k, include_metadata=True)\n",
    "    return [m.metadata[\"text\"] for m in res.matches]\n",
    "\n",
    "\n",
    "# ════════════════ GEMINI PRO RAG ════════════════\n",
    "gemini = genai.GenerativeModel(model_name=\"gemini-2.0-flash\") \n",
    "\n",
    "# Plantilla — guárdala como PROMPT_TEMPLATE\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Eres un/a **abogado/a constitucionalista argentino/a**.  \n",
    "Tu tarea es **contestar en UNA sola frase** y **exclusivamente** con la\n",
    "información que aparece dentro de las etiquetas <context></context>.\n",
    "\n",
    "Reglas de oro (cúmplelas al pie de la letra):\n",
    "\n",
    "1. Si la respuesta está en el contexto, da la solución **exactamente** como\n",
    "   figura allí, sin agregar ni quitar nada relevante.\n",
    "2. Al final de la frase, escribe entre paréntesis el/los número(s) de\n",
    "   artículo(s) que sustenten la respuesta –por ejemplo: **(art. 14)**.\n",
    "   - Si el fragmento de contexto trae algo como “Artículo 14 bis”, ponlo igual: **(art. 14 bis)**.\n",
    "3. Si la información **no** aparece en el contexto, contesta **exactamente**:\n",
    "   > No tengo información sobre esto.\n",
    "4. No inventes datos, no cites fuentes externas, no expliques tu razonamiento.\n",
    "5. Responde en español neutro y evita tecnicismos innecesarios.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Pregunta: {question}\n",
    "Respuesta:\n",
    "\"\"\".strip()\n",
    "\n",
    "def rag_answer(question: str) -> str:\n",
    "    context = \"\\n\\n\".join(retrieve(question))\n",
    "    prompt  = PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "    return gemini.generate_content(prompt).text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Pregunta: ¿Cuáles son las atribuciones del presidente de la Argentina?\n",
      "\n",
      "🧠 Respuesta (Gemini):\n",
      " Es el jefe supremo de la Nación, jefe del gobierno y responsable político de la administración general del país (sin número de artículo).\n"
     ]
    }
   ],
   "source": [
    "# ════════════════ TEST IT ════════════════\n",
    "q = \"¿Cuáles son las atribuciones del presidente de la Argentina?\"\n",
    "print(\"\\n🔎 Pregunta:\", q)\n",
    "print(\"\\n🧠 Respuesta (Gemini):\\n\", rag_answer(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f761793",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY        = \"AIzaSyCAXNRkdO0REJJ66-8u2ubQKMgco9NmaXc\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧪 Procesando fáciles: 100%|██████████| 50/50 [04:34<00:00,  5.48s/it]\n",
      "🧪 Procesando difíciles: 100%|██████████| 65/65 [05:50<00:00,  5.39s/it]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 95.41ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 624.15ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "410517"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from ragas.evaluation import evaluate\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def load_reference_answers(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return {item[\"question\"]: item[\"answer\"] for item in data}\n",
    "\n",
    "\n",
    "\n",
    "# ════════════════════════════════════════════════\n",
    "# Construcción del dataset para RAGAS\n",
    "# ════════════════════════════════════════════════\n",
    "def prepare_ragas_dataset(reference_answers: dict, tag: str = \"\") -> Dataset:\n",
    "    records = []\n",
    "    for question, reference in tqdm(reference_answers.items(), desc=f\"🧪 Procesando {tag}\"):\n",
    "        retrieved = retrieve(question)\n",
    "        answer = rag_answer(question)\n",
    "        time.sleep(4)\n",
    "        records.append({\n",
    "            \"question\": question,\n",
    "            \"contexts\": retrieved,\n",
    "            \"answer\": answer,\n",
    "            \"reference\": reference\n",
    "        })\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "\n",
    "reference_easy = load_reference_answers(\"../easy_questions.json\")\n",
    "reference_hard = load_reference_answers(\"../hard_questions.json\")\n",
    "\n",
    "# Prepare datasets\n",
    "dataset_easy = prepare_ragas_dataset(reference_easy, tag=\"fáciles\")\n",
    "dataset_hard = prepare_ragas_dataset(reference_hard, tag=\"difíciles\")\n",
    "\n",
    "# Save to JSON\n",
    "dataset_easy.to_json(\"ragas_easy.json\", orient=\"records\", lines=False)\n",
    "dataset_hard.to_json(\"ragas_hard.json\", orient=\"records\", lines=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.embeddings.base import LangchainEmbeddingsWrapper\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class CustomE5Embedding(LangchainEmbeddingsWrapper):\n",
    "    def __init__(self, model_name=\"dariolopez/bge-m3-es-legal-tmp-6\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_query(self, texts):\n",
    "        # E5 expects queries to be prefixed like this\n",
    "        texts = [f\"query: {text}\" for text in texts]\n",
    "        return self.model.encode(texts, convert_to_tensor=False)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        # E5 expects docs to be prefixed like this\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        return self.model.encode(texts, convert_to_tensor=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    LLMContextPrecisionWithoutReference,\n",
    "    LLMContextPrecisionWithReference,\n",
    "    NonLLMContextPrecisionWithReference,\n",
    "\n",
    "\n",
    "    ResponseRelevancy,\n",
    "    LLMContextRecall,\n",
    "    Faithfulness\n",
    ")\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.embeddings.base import embedding_factory\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "# Load datasets\n",
    "ragas_easy:Dataset = load_dataset(\"json\", data_files=\"ragas_easy.json\", split=\"train\")\n",
    "ragas_hard:Dataset = load_dataset(\"json\", data_files=\"ragas_hard.json\", split=\"train\")\n",
    "\n",
    "# Run config\n",
    "run_config = RunConfig()\n",
    "custom_embeddings = CustomE5Embedding() \n",
    "\n",
    "metrics = [\n",
    "    LLMContextPrecisionWithReference(),\n",
    "    LLMContextRecall(),\n",
    "    Faithfulness()\n",
    "]\n",
    "\n",
    "print(\"\\n📊 Evaluando preguntas FÁCILES:\")\n",
    "result_easy = evaluate(\n",
    "    ragas_easy,\n",
    "    metrics=metrics,\n",
    "    run_config=run_config,\n",
    "    batch_size=1\n",
    ")\n",
    "print(\"✅ Resultados EASY:\", result_easy)\n",
    "\n",
    "print(\"\\n📊 Evaluando preguntas DIFÍCILES:\")\n",
    "result_hard = evaluate(\n",
    "    ragas_hard,\n",
    "    metrics=metrics,\n",
    "    run_config=run_config,\n",
    "    batch_size=1\n",
    ")\n",
    "print(\"✅ Resultados HARD:\", result_hard)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e426934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# CSV path\n",
    "csv_path = Path(\"../results.csv\")\n",
    "write_header = not csv_path.exists()\n",
    "\n",
    "# Define metrics you expect\n",
    "metric_names = [\n",
    "    \"llm_context_precision_with_reference\",\n",
    "    \"context_recall\",\n",
    "    \"faithfulness\"\n",
    "]\n",
    "\n",
    "# Compute means manually from EvaluationResult\n",
    "easy_scores = [np.mean(result_easy[m]) for m in metric_names]\n",
    "hard_scores = [np.mean(result_hard[m]) for m in metric_names]\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_path, mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    if write_header:\n",
    "        writer.writerow([\"experiment\", \"dataset\"] + metric_names)\n",
    "\n",
    "    writer.writerow([\"experiment_4\", \"easy\"] + easy_scores)\n",
    "    writer.writerow([\"experiment_4\", \"hard\"] + hard_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "juegos-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
