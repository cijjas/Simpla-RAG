{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load .env from parent directory if needed\n",
    "env_path = Path(__file__).resolve().parent.parent / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Access variables\n",
    "OPENAI_API_KEY        = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_INDEX_NAME   = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "PINECONE_HOST         = os.getenv(\"PINECONE_HOST\")\n",
    "PINECONE_API_KEY      = os.getenv(\"PINECONE_API_KEY\")\n",
    "GEMINI_API_KEY        = os.getenv(\"GEMINI_API_KEY\")\n",
    "K_RETRIEVE            = int(os.getenv(\"K_RETRIEVE\", 5))  # default to 5\n",
    "OPENAI_API_KEY        = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import pinecone\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê INIT MODELS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "model = SentenceTransformer(\"dariolopez/bge-m3-es-legal-tmp-6\")  # 1024-D\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê LOAD CORPUS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "def load_articulos(file_path: str) -> List[str]:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "ARTICULOS = load_articulos(\"articulos.txt\")\n",
    "print(f\"üìö  Loaded {len(ARTICULOS):,} art√≠culos\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê EMBEDDING FUNCTION (E5) ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    formatted = [f\"passage: {text}\" for text in texts]\n",
    "    return model.encode(formatted, show_progress_bar=True)\n",
    "\n",
    "print(\"üîß  Generating embeddings ‚Ä¶\")\n",
    "EMBEDS = embed_texts(ARTICULOS)\n",
    "assert len(EMBEDS[0]) == 1024, \"‚ùå Embedding dim mismatch!\"\n",
    "\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê PINECONE SETUP ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(name=PINECONE_INDEX_NAME, host=PINECONE_HOST)\n",
    "\n",
    "def upsert_vectors(texts: List[str],\n",
    "                   vecs: List[List[float]],\n",
    "                   batch: int = 100):\n",
    "    for i in tqdm(range(0, len(texts), batch), desc=\"‚¨ÜÔ∏è  Upserting\"):\n",
    "        batch_vecs = [\n",
    "            {\n",
    "                \"id\": f\"id-{j}\",\n",
    "                \"values\": vecs[j],\n",
    "                \"metadata\": {\"text\": texts[j]}\n",
    "            }\n",
    "            for j in range(i, min(i + batch, len(texts)))\n",
    "        ]\n",
    "        index.upsert(vectors=batch_vecs)\n",
    "\n",
    "print(\"Uploading to Pinecone ‚Ä¶\")\n",
    "upsert_vectors(ARTICULOS, EMBEDS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê RETRIEVE FUNCTION  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "def retrieve(query: str, k: int = K_RETRIEVE) -> List[str]:\n",
    "    query_vec = model.encode(f\"query: {query}\")\n",
    "    res = index.query(vector=query_vec.tolist(), top_k=k, include_metadata=True)\n",
    "    return [m.metadata[\"text\"] for m in res.matches]\n",
    "\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê GEMINI PRO RAG ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "gemini = genai.GenerativeModel(model_name=\"gemini-2.0-flash\") \n",
    "\n",
    "def rag_answer(question: str) -> str:\n",
    "    context = \"\\n\\n\".join(retrieve(question))\n",
    "    prompt  = f\"Contexto:\\n{context}\\n\\nPregunta: {question}\\nRespuesta:\"\n",
    "    return gemini.generate_content(prompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê TEST IT ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "q = \"¬øCu√°les son las atribuciones del presidente de la Argentina?\"\n",
    "print(\"\\nüîé Pregunta:\", q)\n",
    "print(\"\\nüß† Respuesta (Gemini):\\n\", rag_answer(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ragas.evaluation import evaluate\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def load_reference_answers(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return {item[\"question\"]: item[\"answer\"] for item in data}\n",
    "\n",
    "\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Construcci√≥n del dataset para RAGAS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "def prepare_ragas_dataset(reference_answers: dict, tag: str = \"\") -> Dataset:\n",
    "    records = []\n",
    "    for question, reference in tqdm(reference_answers.items(), desc=f\"üß™ Procesando {tag}\"):\n",
    "        retrieved = retrieve(question)\n",
    "        answer = rag_answer(question)\n",
    "        records.append({\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": retrieved,\n",
    "            \"response\": answer,\n",
    "            \"reference\": reference\n",
    "        })\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "\n",
    "reference_easy = load_reference_answers(\"../easy_questions.json\")\n",
    "reference_hard = load_reference_answers(\"../hard_questions.json\")\n",
    "\n",
    "dataset_easy = prepare_ragas_dataset(reference_easy, tag=\"f√°ciles\")\n",
    "dataset_hard = prepare_ragas_dataset(reference_hard, tag=\"dif√≠ciles\")\n",
    "\n",
    "dataset_easy.to_json(\"ragas_easy.json\", orient=\"records\", lines=False)\n",
    "dataset_hard.to_json(\"ragas_hard.json\", orient=\"records\", lines=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.embeddings.base import LangchainEmbeddingsWrapper\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class CustomE5Embedding(LangchainEmbeddingsWrapper):\n",
    "    def __init__(self, model_name=\"dariolopez/bge-m3-es-legal-tmp-6\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_query(self, texts):\n",
    "        texts = [f\"query: {text}\" for text in texts]\n",
    "        return self.model.encode(texts, convert_to_tensor=False)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        return self.model.encode(texts, convert_to_tensor=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    LLMContextPrecisionWithoutReference,\n",
    "    LLMContextPrecisionWithReference,\n",
    "    NonLLMContextPrecisionWithReference,\n",
    "\n",
    "\n",
    "    ResponseRelevancy,\n",
    "    LLMContextRecall,\n",
    "    Faithfulness\n",
    ")\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.embeddings.base import embedding_factory\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "# Load datasets\n",
    "ragas_easy:Dataset = load_dataset(\"json\", data_files=\"ragas_easy.json\", split=\"train\")\n",
    "ragas_hard:Dataset = load_dataset(\"json\", data_files=\"ragas_hard.json\", split=\"train\")\n",
    "\n",
    "# Run config\n",
    "run_config = RunConfig()\n",
    "custom_embeddings = CustomE5Embedding() \n",
    "\n",
    "metrics = [\n",
    "    LLMContextPrecisionWithReference(),\n",
    "    LLMContextRecall(),\n",
    "    Faithfulness()\n",
    "]\n",
    "\n",
    "print(\"\\nüìä Evaluando preguntas F√ÅCILES:\")\n",
    "result_easy = evaluate(\n",
    "    ragas_easy,\n",
    "    metrics=metrics,\n",
    "    run_config=run_config,\n",
    "    batch_size=1\n",
    ")\n",
    "print(\"‚úÖ Resultados EASY:\", result_easy)\n",
    "\n",
    "print(\"\\nüìä Evaluando preguntas DIF√çCILES:\")\n",
    "result_hard = evaluate(\n",
    "    ragas_hard,\n",
    "    metrics=metrics,\n",
    "    run_config=run_config,\n",
    "    batch_size=1\n",
    ")\n",
    "print(\"‚úÖ Resultados HARD:\", result_hard)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e426934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "csv_path = Path(\"../results.csv\")\n",
    "write_header = not csv_path.exists()\n",
    "\n",
    "metric_names = [\n",
    "    \"llm_context_precision_with_reference\",\n",
    "    \"context_recall\",\n",
    "    \"faithfulness\"\n",
    "]\n",
    "\n",
    "easy_scores = [np.mean(result_easy[m]) for m in metric_names]\n",
    "hard_scores = [np.mean(result_hard[m]) for m in metric_names]\n",
    "\n",
    "with open(csv_path, mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    if write_header:\n",
    "        writer.writerow([\"experiment\", \"dataset\"] + metric_names)\n",
    "\n",
    "    writer.writerow([\"experiment_5\", \"easy\"] + easy_scores)\n",
    "    writer.writerow([\"experiment_5\", \"hard\"] + hard_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "juegos-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
